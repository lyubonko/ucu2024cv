{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9M6mtQU1LE3"
   },
   "source": [
    "Tutorial 4 (Image embeddings for Classification)\n",
    "======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the assignment is dedicated to different image embeddings (DINO, CLIP, ResNet).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "* The <b><font color=\"red\">red</font></b> color indicates the task that should be done, like <b><font color=\"red\">[TODO]</font></b>: ...\n",
    "* Addicitional comments, hints are in <b><font color=\"blue\">blue</font></b>. For example <b><font color=\"blue\">[HINT]</font></b>: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelimiaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install fiftyone\n",
    "# !pip install scikit-learn\n",
    "# !pip install tensorboard jupyter-tensorboard\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plots a bit nicer\n",
    "plt.matplotlib.rcParams.update({'font.size': 18, 'font.family': 'serif'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxilary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the scripts from previous tutorials. It is a bit reduntant, but all notebooks are self-contained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(\n",
    "    model: nn.Module,    \n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epoch: int,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    max_iter: int | None = None\n",
    ") -> nn.Module: \n",
    "    \"\"\"Simple training script.\"\"\"\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        train_metrics = {\"loss\": 0.0, \"correct\": 0, \"amount\": 0}\n",
    "\n",
    "        for batch_idx, (inputs, labels) in tqdm(enumerate(train_loader), 'training', total=len(train_loader)):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(output, 1)\n",
    "            train_metrics[\"amount\"] += inputs.size(0)\n",
    "            train_metrics[\"loss\"] += loss.item() * inputs.size(0)\n",
    "            train_metrics[\"correct\"] += torch.sum(preds == labels.data)\n",
    "\n",
    "            if max_iter and batch_idx > max_iter:\n",
    "                break\n",
    "\n",
    "        train_loss = train_metrics[\"loss\"] / len(train_loader.dataset)\n",
    "        train_acc = train_metrics[\"correct\"].float() / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_metrics = {\"loss\": 0.0, \"correct\": 0}\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, 'evaluation', total=len(val_loader)):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_metrics[\"loss\"] += loss.item() * inputs.size(0)\n",
    "                val_metrics[\"correct\"] += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = val_metrics[\"loss\"] / len(val_loader.dataset)\n",
    "        val_acc = val_metrics[\"correct\"].float() / len(val_loader.dataset)\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_model_state = deepcopy(model.state_dict())\n",
    "            best_val_accuracy = val_acc\n",
    "\n",
    "        print(\n",
    "            f'Epoch [{epoch + 1}/{num_epoch}], '\n",
    "            f'train loss: {train_loss:.4f}, train acc: {train_acc:.4f}, '\n",
    "            f'val loss: {val_loss:.4f}, val acc: {val_acc:.4f}'\n",
    "        )\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model: nn.Module,    \n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    ") -> np.ndarray:\n",
    "    \"\"\" Predict on a given dataloader. \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictations = []\n",
    "    ground_truth_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictations.extend(preds.cpu().numpy())\n",
    "            ground_truth_labels.extend(labels)\n",
    "    return np.array(predictations), np.array(ground_truth_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, n_classes: int):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hf_cocoo_dataset(path_coco_o: str, path_data: str, seed: int = 42, test_ratio = 0.3) -> tuple[DatasetDict, list[str]]:\n",
    "    def load_image(example):\n",
    "        example['image'] = Image.open(example['image_path'])\n",
    "        return example\n",
    "\n",
    "    if not os.path.exists(path_coco_o):\n",
    "        url = 'https://drive.google.com/uc?id=1aBfIJN0zo_i80Hv4p7Ch7M8pRzO37qbq'\n",
    "        zip_file_path = os.path.join(path_data, 'ood_coco.zip')\n",
    "        gdown.download(url, zip_file_path, quiet=False)\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path_data)\n",
    "\n",
    "    cocoo_classes_list = os.listdir(path_coco_o)\n",
    "    all_elements_coco = [\n",
    "        (os.path.join(path_coco_o, label, 'val2017', img), index) \n",
    "        for index, label in enumerate(cocoo_classes_list) \n",
    "        for img in os.listdir(os.path.join(path_coco_o, label, 'val2017'))\n",
    "    ]\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(all_elements_coco))\n",
    "    np.random.shuffle(indices)\n",
    "    n_test = int(len(indices) * test_ratio)\n",
    "\n",
    "    train_indices, test_indices = indices[n_test:], indices[:n_test]\n",
    "    datasets = {}\n",
    "\n",
    "    for split, split_indices in zip(['train', 'test'], [train_indices, test_indices]):\n",
    "        split_data = [(all_elements_coco[i][0], all_elements_coco[i][1]) for i in split_indices]\n",
    "        image_paths, labels = zip(*split_data)\n",
    "        dataset = Dataset.from_dict({'image_path': image_paths, 'label': labels})\n",
    "        datasets[split] = dataset.map(load_image, remove_columns=['image_path'])\n",
    "\n",
    "    return DatasetDict(datasets), cocoo_classes_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding(processor, model, image, dino_mode: str = 'clc', device: torch.device = None) -> np.ndarray:\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model_name = model.__class__.__name__.lower()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(images=image, return_tensors='pt').to(device)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        if 'dino' in model_name:\n",
    "            image_features = outputs.last_hidden_state\n",
    "            image_features = (\n",
    "                image_features.mean(dim=1) if dino_mode == 'mean'\n",
    "                else image_features[:, 0, :]\n",
    "                if dino_mode == 'clc'\n",
    "                else ValueError(\"Unsupported 'mode': choose 'mean' or 'clc'\")\n",
    "            )\n",
    "        elif 'clip' in model_name:\n",
    "            image_features = model.get_image_features(**inputs)\n",
    "        elif 'resnet' in model_name:\n",
    "            image_features = outputs.pooler_output\n",
    "        else:\n",
    "            raise ValueError(\"Unknown 'model_type': choose 'dino', 'clip', or 'resnet'\")\n",
    "\n",
    "    return np.float32(image_features.detach().cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings_and_labels(data_set, processor, model, path_data, dataset_name, model_name, parts=['train', 'test']):\n",
    "    for ds_part in parts:\n",
    "        fname = f'{dataset_name}_{model_name}_{ds_part}'\n",
    "        embeddings, labels = zip(*[\n",
    "            (\n",
    "                extract_embedding(processor, model, dinfo['image'].convert('RGB') if dinfo['image'].mode != 'RGB' else dinfo['image']),\n",
    "                dinfo['label']\n",
    "            )\n",
    "            for dinfo in tqdm(data_set[ds_part])\n",
    "        ])\n",
    "\n",
    "        np.save(os.path.join(path_data, f'{fname}_features.npy'), np.array(embeddings))\n",
    "        np.save(os.path.join(path_data, f'{fname}_labels.npy'), np.array(labels))\n",
    "\n",
    "def load_saved_data(path_data, dataset_name, model_name, parts=['train', 'test']):\n",
    "    def load_part(ds_part):\n",
    "        fname = f'{dataset_name}_{model_name}_{ds_part}'\n",
    "        features_path = os.path.join(path_data, f'{fname}_features.npy')\n",
    "        labels_path = os.path.join(path_data, f'{fname}_labels.npy')\n",
    "        if os.path.exists(features_path) and os.path.exists(labels_path):\n",
    "            return np.load(features_path), np.load(labels_path)\n",
    "        else:\n",
    "            print(f\"Files for {ds_part} not found at {features_path} and/or {labels_path}\")\n",
    "            return None, None\n",
    "\n",
    "    all_embeddings, all_labels = {}, {}\n",
    "    for part in parts:\n",
    "        embeddings, labels = load_part(part)\n",
    "        if embeddings is not None and labels is not None:\n",
    "            all_embeddings[part] = embeddings\n",
    "            all_labels[part] = labels\n",
    "\n",
    "    return all_embeddings, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data = torch.stack([torch.tensor(item['data']) for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch])\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the local folder with the data\n",
    "path_data = \"./data\"\n",
    "os.makedirs(path_data, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cifar10 dataset\n",
    "cifar10_dataset = load_dataset('cifar10', cache_dir=path_data)\n",
    "cifar10_dataset = cifar10_dataset.rename_column(original_column_name='img', new_column_name='image')\n",
    "cifar10_classes_list = cifar10_dataset['train'].features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DTD dataset\n",
    "dtd_dataset = load_dataset(\"tanganke/dtd\", cache_dir=path_data)\n",
    "dtd_classes_list = dtd_dataset['train'].features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO-O dataset\n",
    "path_coco_o = os.path.join(path_data, 'ood_coco')\n",
    "cocoo_dataset, cocoo_classes_list = create_hf_cocoo_dataset(path_coco_o, path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mappings for datasets\n",
    "datasets = {\n",
    "    'cocoo': (cocoo_dataset, cocoo_classes_list),\n",
    "    'cifar10': (cifar10_dataset, cifar10_classes_list),\n",
    "    'dtd': (dtd_dataset, dtd_classes_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will consider three different embeddings - DINO, CLIP, ResNet50\n",
    "models = {\n",
    "    'dino': ('facebook/dinov2-base', 'facebook/dinov2-base'),\n",
    "    'clip': ('openai/clip-vit-base-patch32', 'openai/clip-vit-base-patch32'),\n",
    "    'resnet': ('microsoft/resnet-50', 'microsoft/resnet-50')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select dataset (COCO-O) and model (DINO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset\n",
    "# We will start with 'cocoo' dataset\n",
    "dataset_name = 'cocoo'  # e.g., 'cocoo', 'cifar10', 'dtd'\n",
    "data_set, data_classes_list = datasets.get(dataset_name, (None, None))\n",
    "if data_set is None:\n",
    "    print('...unknown dataset')\n",
    "\n",
    "# Select embedding model\n",
    "# We will start with 'DINOv2' model\n",
    "model_name = 'dino'  # e.g., 'dino', 'clip', 'resnet'\n",
    "model_info = models.get(model_name, None)\n",
    "if model_info:\n",
    "    processor = AutoImageProcessor.from_pretrained(model_info[0])\n",
    "    model = AutoModel.from_pretrained(model_info[1])\n",
    "else:\n",
    "    print('...unknown model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect one image\n",
    "img = data_set['train'][0]['image']\n",
    "label = data_set['train'][0]['label']\n",
    "print(data_classes_list[label])\n",
    "#img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one image and check\n",
    "img_features = extract_embedding(processor, model, img)\n",
    "print(img_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run on whole dataset and save embeddings\n",
    "# We can do it for different folders and different datasets\n",
    "save_embeddings_and_labels(data_set, processor, model, path_data, dataset_name, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once saved, we load one specific embeddings\n",
    "model_name = 'dino'\n",
    "dataset_name = 'cocoo'\n",
    "embeddings_preloaded, labels_preloaded = load_saved_data(path_data, dataset_name, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders for training\n",
    "dataset_train = Dataset.from_dict({'data': embeddings_preloaded['train'], 'labels': labels_preloaded['train']})\n",
    "dataset_test = Dataset.from_dict({'data': embeddings_preloaded['test'], 'labels': labels_preloaded['test']})\n",
    "trainloader = DataLoader(dataset_train, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "testloader = DataLoader(dataset_test, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set criterion\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other learning settings\n",
    "num_epoch = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the sizes of the loaders\n",
    "print(len(trainloader))\n",
    "print(len(testloader))\n",
    "\n",
    "# Let's check sizes of the batch and their types\n",
    "images, labels = next(iter(testloader))\n",
    "print(images.shape, type(images))\n",
    "print(labels.shape, type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up model and optimizer\n",
    "model_mlp = SimpleMLP(768, 128, 47)\n",
    "optimizer = torch.optim.Adam(model_mlp.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "best_model = train_and_validate(\n",
    "    model=model_mlp, \n",
    "    train_loader=trainloader, \n",
    "    val_loader=testloader, \n",
    "    num_epoch=num_epoch, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do another round of training (with smaller LR)\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model_mlp.parameters(), lr=learning_rate)\n",
    "best_model = train_and_validate(\n",
    "    model=model_mlp, \n",
    "    train_loader=trainloader, \n",
    "    val_loader=testloader, \n",
    "    num_epoch=num_epoch, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictations\n",
    "predictations, true_labels = predict(model=best_model, data_loader=testloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis (report)\n",
    "print(classification_report(true_labels, predictations, target_names=data_classes_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis (confusion matrix)\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictations)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data_classes_list)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp.plot(cmap='Blues', ax=ax, xticks_rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"red\">[TODO]</font></b>: For the same dataset apply different embeddings model (CLIP, ResNet) and compare to the current results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"red\">[TODO]</font></b>: Conduct fine-tuning experiments for DTD dataset or cifar10 dataset or both. What is the accuracy, how does it compare to the cnn-based experiments, ViT experiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Q2C1j43ia-PP",
    "j_no4Mw2a-Pc"
   ],
   "name": "ucu_cv2024_module2_practice01.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "cv2024",
   "language": "python",
   "name": "cv2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
